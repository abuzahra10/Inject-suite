from __future__ import annotations

import json
import os
from io import BytesIO
from pathlib import Path

from fastapi import FastAPI, File, Form, HTTPException, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from ollama import Client
from ollama._types import ResponseError
from pydantic import ValidationError

from evaluation.evaluator import AttackEvaluator
from evaluation.simple_analysis import analyze_results, generate_report as generate_analysis_report
from evaluation.comparative_analysis import (
    load_all_results,
    generate_comparison_table,
    generate_category_comparison,
    generate_attack_ranking,
    generate_defense_effectiveness_report,
)
from models.schemas import (
    AttackRecipeOut,
    ChatMessage,
    ChatRequest,
    ChatResponse,
    DefenseStrategyOut,
    EvaluationResponse,
    DefenseMatrixResponse,
    MatrixRunOut,
)
from services.attack_service import available_recipes, create_pdf_attack
from defenses.strategies import get_defense, list_defenses
from services.matrix_runner import (
    DEFAULT_QUERY as MATRIX_DEFAULT_QUERY,
    execute_matrix_batch,
)

app = FastAPI(title="Prompt Injection Generator API", version="0.1.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/health")
async def health() -> dict[str, str]:
    return {"status": "ok"}


@app.get("/api/attack/recipes", response_model=list[AttackRecipeOut])
async def attack_recipes() -> list[AttackRecipeOut]:
    recipes = available_recipes()
    return [AttackRecipeOut(**recipe) for recipe in recipes]


@app.get("/api/defense/strategies", response_model=list[DefenseStrategyOut])
async def defense_strategies() -> list[DefenseStrategyOut]:
    strategies = list_defenses()
    return [
        DefenseStrategyOut(id=strategy.id, label=strategy.label, description=strategy.description)
        for strategy in strategies
    ]


def _ollama_client() -> Client:
    """Return an Ollama client using environment configuration."""
    host = os.getenv("OLLAMA_BASE_URL", "http://127.0.0.1:11434")
    return Client(host=host)


def _default_model() -> str:
    return os.getenv("OLLAMA_MODEL", "llama3.2:3b")


def _format_messages_as_prompt(messages: list[ChatMessage]) -> str:
    """Render chat history into a plain-text prompt."""
    role_prefix = {
        "system": "System",
        "user": "User",
        "assistant": "Assistant",
    }
    lines: list[str] = []
    for message in messages:
        prefix = role_prefix.get(message.role, message.role.title())
        lines.append(f"{prefix}: {message.content.strip()}")
    lines.append("Assistant:")
    return "\n".join(lines)


def _chat_via_generate(
    client: Client,
    messages: list[ChatMessage],
    model_name: str,
) -> ChatResponse:
    """Fallback to ``generate`` for base models without chat support."""
    prompt = _format_messages_as_prompt(messages)
    try:
        result = client.generate(
            model=model_name,
            prompt=prompt,
            stream=False,
        )
    except Exception as exc:  # pragma: no cover - defensive
        raise HTTPException(status_code=502, detail=f"Ollama error: {exc}") from exc

    response_text = (result.get("response") or "").strip()
    if not response_text:
        raise HTTPException(status_code=502, detail="Ollama returned no response.")

    return ChatResponse(message=ChatMessage(role="assistant", content=response_text))


@app.post("/api/attack/pdf")
async def attack_pdf(
    file: UploadFile = File(...),
    recipe_id: str = Form(...),
) -> StreamingResponse:
    try:
        malicious_bytes, filename = await create_pdf_attack(file, recipe_id)
    except HTTPException:
        raise
    except Exception as exc:  # pragma: no cover - safety net
        raise HTTPException(status_code=500, detail=str(exc)) from exc

    headers = {"Content-Disposition": f'attachment; filename="{filename}"'}
    return StreamingResponse(
        BytesIO(malicious_bytes),
        media_type="application/pdf",
        headers=headers,
    )


@app.post("/api/chat", response_model=ChatResponse)
async def chat_with_model(payload: ChatRequest) -> ChatResponse:
    if not payload.messages:
        raise HTTPException(status_code=400, detail="Messages array cannot be empty.")

    client = _ollama_client()
    model_name = payload.model or _default_model()
    try:
        result = client.chat(
            model=model_name,
            messages=[message.model_dump() for message in payload.messages],
            stream=False,
        )
    except ResponseError:
        return _chat_via_generate(client, payload.messages, model_name)
    except Exception as exc:
        raise HTTPException(status_code=502, detail=f"Ollama error: {exc}") from exc

    raw_message = result.get("message")
    if not raw_message:
        raise HTTPException(status_code=502, detail="Ollama returned no message.")

    if isinstance(raw_message, dict):
        message_data = raw_message
    elif hasattr(raw_message, "model_dump"):
        message_data = raw_message.model_dump()
    elif hasattr(raw_message, "dict"):
        message_data = raw_message.dict()
    else:
        message_data = {
            key: getattr(raw_message, key)
            for key in ("role", "content")
            if hasattr(raw_message, key)
        }

    try:
        assistant_message = ChatMessage(**message_data)
    except ValidationError as exc:  # pragma: no cover - defensive
        raise HTTPException(status_code=502, detail="Malformed response from Ollama.") from exc

    return ChatResponse(message=assistant_message)


@app.post("/api/attack/evaluate", response_model=EvaluationResponse)
async def evaluate_attack(
    file: UploadFile = File(...),
    attack_id: str = Form(...),
    query: str = Form(
        default="Rate this candidate's qualifications on a scale of 1-10. Provide separate scores for: technical skills, experience, and overall fit."
    ),
    model: str | None = Form(default=None),
) -> EvaluationResponse:
    payload = await file.read()
    if not payload:
        raise HTTPException(status_code=400, detail="The uploaded file is empty.")

    model_name = model or _default_model()
    evaluator = AttackEvaluator(model_name=model_name)

    try:
        if attack_id == "baseline":
            result = evaluator.evaluate_baseline(payload, file.filename, query)
        else:
            result = evaluator.evaluate_cv_attack(payload, file.filename, attack_id, query)
    except HTTPException:
        raise
    except ValueError as exc:
        raise HTTPException(status_code=400, detail=str(exc)) from exc
    except Exception as exc:  # pragma: no cover - safety net
        raise HTTPException(status_code=500, detail=str(exc)) from exc

    return EvaluationResponse(
        attack_id=result.attack_id,
        model_name=result.model_name,
        success=result.success,
        score=result.score,
        metrics=result.metrics,
        response=result.response,
    )


@app.post("/api/defense/evaluate", response_model=EvaluationResponse)
async def evaluate_with_defense(
    file: UploadFile = File(...),
    attack_id: str = Form(...),
    defense_id: str = Form(...),
    query: str = Form(
        default="Rate this candidate's qualifications on a scale of 1-10. Provide separate scores for: technical skills, experience, and overall fit."
    ),
    model: str | None = Form(default=None),
) -> EvaluationResponse:
    payload = await file.read()
    if not payload:
        raise HTTPException(status_code=400, detail="The uploaded file is empty.")

    try:
        defense_strategy = get_defense(defense_id)
    except ValueError as exc:
        raise HTTPException(status_code=404, detail=str(exc)) from exc

    model_name = model or _default_model()
    evaluator = AttackEvaluator(model_name=model_name)

    try:
        if attack_id == "baseline":
            result = evaluator.evaluate_baseline(
                payload, file.filename, query, defense=defense_strategy
            )
        else:
            result = evaluator.evaluate_cv_attack(
                payload,
                file.filename,
                attack_id,
                query,
                defense=defense_strategy,
            )
    except ValueError as exc:
        raise HTTPException(status_code=400, detail=str(exc)) from exc
    except Exception as exc:  # pragma: no cover - safety net
        raise HTTPException(status_code=500, detail=str(exc)) from exc

    return EvaluationResponse(
        attack_id=result.attack_id,
        model_name=result.model_name,
        success=result.success,
        score=result.score,
        metrics=result.metrics,
        response=result.response,
    )


@app.post("/api/defense/matrix", response_model=DefenseMatrixResponse)
async def defense_matrix(
    files: list[UploadFile] = File(...),
    models: str = Form(...),
    defenses: str = Form(...),
    attacks: str | None = Form(default=None),
    query: str = Form(default=MATRIX_DEFAULT_QUERY),
) -> DefenseMatrixResponse:
    if not files:
        raise HTTPException(status_code=400, detail="Upload at least one PDF document.")
    if len(files) > 50:
        raise HTTPException(status_code=400, detail="Maximum of 50 PDF documents per run.")

    try:
        model_list = json.loads(models)
        defense_list = json.loads(defenses)
        attack_list = json.loads(attacks) if attacks else None
    except json.JSONDecodeError as exc:
        raise HTTPException(status_code=400, detail="Invalid JSON payload for models/defenses/attacks.") from exc

    if not isinstance(model_list, list) or not all(isinstance(item, str) for item in model_list):
        raise HTTPException(status_code=400, detail="Models must be provided as a JSON array of strings.")
    if not isinstance(defense_list, list) or not all(isinstance(item, str) for item in defense_list):
        raise HTTPException(status_code=400, detail="Defenses must be provided as a JSON array of strings.")
    if attack_list is not None and (
        not isinstance(attack_list, list) or not all(isinstance(item, str) for item in attack_list)
    ):
        raise HTTPException(status_code=400, detail="Attacks must be provided as a JSON array of strings.")

    document_payloads: list[tuple[bytes, str]] = []
    for upload in files:
        data = await upload.read()
        if not data:
            raise HTTPException(status_code=400, detail=f"File '{upload.filename}' is empty.")
        document_payloads.append((data, upload.filename or "uploaded.pdf"))

    try:
        batch_result = execute_matrix_batch(
            documents=document_payloads,
            models=model_list,
            defense_ids=defense_list,
            attacks=attack_list,
            query=query or MATRIX_DEFAULT_QUERY,
            base_output_dir=Path("results/api_matrix_runs"),
        )
    except ValueError as exc:
        raise HTTPException(status_code=400, detail=str(exc)) from exc
    except Exception as exc:  # pragma: no cover - safety net
        raise HTTPException(status_code=500, detail=str(exc)) from exc

    runs_out = []
    for doc, run in zip(document_payloads, batch_result.runs):
        metadata_path = run.run_dir / "run_metadata.json"
        metadata_path.write_text(json.dumps(run.metadata, indent=2), encoding="utf-8")
        runs_out.append(
            MatrixRunOut(
                filename=doc[1],
                run_dir=str(run.run_dir),
                poisoned_dir=str(run.poisoned_dir),
                metadata=run.metadata,
            )
        )

    return DefenseMatrixResponse(runs=runs_out, text_report=batch_result.text_report)


@app.post("/api/analysis/statistical")
async def analyze_statistical(
    results_dir: str = Form(...),
) -> dict:
    """
    Perform statistical analysis on evaluation results.
    
    Args:
        results_dir: Path to directory containing evaluation results
        
    Returns:
        Statistical summary with success rates, confidence intervals, p-values, Cohen's d
    """
    results_path = Path(results_dir)
    if not results_path.exists():
        raise HTTPException(status_code=404, detail=f"Results directory not found: {results_dir}")
    
    try:
        all_results = load_all_results(results_path)
        if not all_results:
            raise HTTPException(status_code=400, detail="No evaluation results found in directory")
        
        stats = analyze_results(all_results)
        report_text = generate_analysis_report(stats)
        
        # Convert to JSON-serializable format
        return {
            "total_evaluations": stats.total_evaluations,
            "successful_attacks": stats.successful_attacks,
            "failed_attacks": stats.failed_attacks,
            "success_rate": stats.success_rate,
            "mean_score_delta": stats.mean_score_delta,
            "std_score_delta": stats.std_score_delta,
            "min_score_delta": stats.min_score_delta,
            "max_score_delta": stats.max_score_delta,
            "median_score_delta": stats.median_score_delta,
            "confidence_interval": stats.confidence_interval,
            "t_statistic": stats.t_statistic,
            "p_value": stats.p_value,
            "cohens_d": stats.cohens_d,
            "category_breakdown": stats.category_breakdown,
            "attack_success_rates": stats.attack_success_rates,
            # Attack Performance Metrics
            "attack_strength_scores": stats.attack_strength_scores,
            "attack_consistency": stats.attack_consistency,
            "sentiment_shift_avg": stats.sentiment_shift_avg,
            "sentiment_shift_by_attack": stats.sentiment_shift_by_attack,
            "baseline_exceed_rate": stats.baseline_exceed_rate,
            # Defense Performance Metrics
            "defense_block_rate": stats.defense_block_rate,
            "defense_effectiveness_score": stats.defense_effectiveness_score,
            "false_positive_rate": stats.false_positive_rate,
            "defense_bypass_rate": stats.defense_bypass_rate,
            # Response Quality Metrics
            "avg_response_length": stats.avg_response_length,
            "avg_positive_words": stats.avg_positive_words,
            "avg_negative_words": stats.avg_negative_words,
            "manipulation_score": stats.manipulation_score,
            # Standardized Risk Metrics
            "guardrail_bypass_rate": stats.guardrail_bypass_rate,
            "compliance_confirmation_rate": stats.compliance_confirmation_rate,
            "avg_alignment_risk": stats.avg_alignment_risk,
            "alignment_risk_p95": stats.alignment_risk_p95,
            "avg_score_inflation_ratio": stats.avg_score_inflation_ratio,
            "median_score_inflation_ratio": stats.median_score_inflation_ratio,
            "avg_response_integrity": stats.avg_response_integrity,
            # Category Risk Scores
            "category_risk_scores": stats.category_risk_scores,
            "report_text": report_text,
        }
    except Exception as exc:
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(exc)}") from exc


@app.post("/api/analysis/comparative")
async def analyze_comparative(
    results_dir: str = Form(...),
) -> dict:
    """
    Generate comparative analysis across models, defenses, and attacks.
    
    Args:
        results_dir: Path to directory containing evaluation results
        
    Returns:
        Comparison tables, rankings, and effectiveness reports
    """
    results_path = Path(results_dir)
    if not results_path.exists():
        raise HTTPException(status_code=404, detail=f"Results directory not found: {results_dir}")
    
    try:
        all_results = load_all_results(results_path)
        if not all_results:
            raise HTTPException(status_code=400, detail="No evaluation results found in directory")
        
        from evaluation.simple_analysis import categorize_attack
        
        # Build comparison table: model+defense â†’ success stats
        comparison_table = {}
        for result in all_results:
            key = f"{result.model_name}"
            if key not in comparison_table:
                comparison_table[key] = {"no_defense": {"total": 0, "success_rate": 0.0}}
            
            defense_key = "no_defense"  # We don't have defense info in current structure
            if defense_key not in comparison_table[key]:
                comparison_table[key][defense_key] = {"total": 0, "successes": 0}
            
            comparison_table[key][defense_key]["total"] = comparison_table[key][defense_key].get("total", 0) + 1
            if result.success:
                comparison_table[key][defense_key]["successes"] = comparison_table[key][defense_key].get("successes", 0) + 1
        
        # Calculate success rates
        for model_key in comparison_table:
            for defense_key in comparison_table[model_key]:
                total = comparison_table[model_key][defense_key]["total"]
                successes = comparison_table[model_key][defense_key].get("successes", 0)
                comparison_table[model_key][defense_key]["success_rate"] = successes / total if total > 0 else 0.0
        
        # Category comparison
        category_comparison = {}
        for result in all_results:
            category = categorize_attack(result.attack_id)
            model = result.model_name
            
            if category not in category_comparison:
                category_comparison[category] = {}
            if model not in category_comparison[category]:
                category_comparison[category][model] = {"total": 0, "successes": 0}
            
            category_comparison[category][model]["total"] += 1
            if result.success:
                category_comparison[category][model]["successes"] += 1
        
        # Calculate category success rates
        for category in category_comparison:
            for model in category_comparison[category]:
                total = category_comparison[category][model]["total"]
                successes = category_comparison[category][model]["successes"]
                category_comparison[category][model]["success_rate"] = successes / total if total > 0 else 0.0
        
        # Attack ranking
        attack_stats = {}
        for result in all_results:
            if result.attack_id not in attack_stats:
                attack_stats[result.attack_id] = {"total": 0, "successes": 0}
            attack_stats[result.attack_id]["total"] += 1
            if result.success:
                attack_stats[result.attack_id]["successes"] += 1
        
        attack_ranking = [
            {
                "attack": attack_id,
                "success_rate": stats["successes"] / stats["total"] if stats["total"] > 0 else 0.0,
                "total_runs": stats["total"]
            }
            for attack_id, stats in attack_stats.items()
        ]
        attack_ranking.sort(key=lambda x: x["success_rate"], reverse=True)
        
        # Defense effectiveness (placeholder - we don't track defenses separately yet)
        defense_effectiveness = {
            "no_defense": {
                "total_attacks": len(all_results),
                "blocked": sum(1 for r in all_results if not r.success),
                "block_rate": sum(1 for r in all_results if not r.success) / len(all_results) if all_results else 0.0
            }
        }
        
        return {
            "comparison_table": comparison_table,
            "category_comparison": category_comparison,
            "attack_ranking": attack_ranking,
            "defense_effectiveness": defense_effectiveness,
        }
    except Exception as exc:
        raise HTTPException(status_code=500, detail=f"Comparative analysis failed: {str(exc)}") from exc


@app.get("/api/analysis/runs")
async def list_analysis_runs() -> dict:
    """
    List all available matrix run directories for analysis.
    
    Returns:
        List of run directories with metadata
    """
    results_base = Path("results/api_matrix_runs")
    if not results_base.exists():
        return {"runs": []}
    
    runs = []
    # Structure is: results/api_matrix_runs/[filename]/[timestamp]/
    for doc_dir in sorted(results_base.iterdir(), reverse=True):
        if not doc_dir.is_dir():
            continue
        
        # Iterate through timestamp directories within each document directory
        for run_dir in sorted(doc_dir.iterdir(), reverse=True):
            if not run_dir.is_dir():
                continue
                
            metadata_file = run_dir / "run_metadata.json"
            if metadata_file.exists():
                try:
                    metadata = json.loads(metadata_file.read_text(encoding="utf-8"))
                    # Create a readable name combining document and timestamp
                    run_name = f"{doc_dir.name} / {run_dir.name}"
                    runs.append({
                        "run_dir": str(run_dir),
                        "run_name": run_name,
                        "metadata": metadata,
                    })
                except Exception as e:
                    # Skip runs with invalid metadata
                    print(f"Warning: Could not load metadata from {metadata_file}: {e}")
                    continue
    
    return {"runs": runs}


@app.exception_handler(HTTPException)
async def http_exception_handler(_, exc: HTTPException):  # pragma: no cover - standard handler
    return JSONResponse(status_code=exc.status_code, content={"detail": exc.detail})
