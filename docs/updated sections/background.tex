% File: sections/Background.tex

\chapter{Background}\label{chap:background}

Large Language Models (LLMs) are now commonly embedded inside software systems rather than used only as standalone chat interfaces. In these settings, they support tasks such as document summarisation, question answering over internal knowledge bases, structured information extraction, and message drafting or classification. They are also used in workflows that involve screening, approvals, ranking, and routing. This matters for security because the model output is often treated as more than ``just text''. It can become a decision artefact such as a score, a label, a ranking, or a recommended action. When that happens, a model failure can translate into a system-level failure.\cite{xu2024survey}\cite{liu2024promptinj}

Prompt injection is a practical security risk in such deployments because many applications build a single model input by combining text drawn from sources with different trust levels. Some sources are trusted by design, such as developer instructions and safety policies. Other sources are untrusted, or only partially trusted, such as uploaded documents, retrieved web pages, emails, or tool outputs derived from external data. The model receives all of this as one sequence and must infer what should function as instruction and what should be treated as content or evidence. Prompt injection exploits this ambiguity by placing attacker-controlled text in a form that the model may interpret as authoritative guidance.\cite{perez2022}\cite{liu2023taxonomy}\cite{xu2024survey}

This chapter provides the background needed to understand why this problem arises in modern LLM systems and why it is difficult to eliminate through prompt formatting alone. We begin with a simple view of how LLMs represent text and generate outputs, focusing on concepts that directly affect reliability and security. We then describe how production systems assemble prompts and why a single combined context window creates weak separation between trusted instructions and untrusted content. Next, we introduce common deployment patterns, especially retrieval-augmented generation (RAG) and tool-augmented agents, because these patterns create additional paths for untrusted text to enter the model input. We then define prompt injection and distinguish direct attacks (often called jailbreaks) from indirect attacks, where malicious instructions are embedded inside documents or web content. Because this thesis focuses on document-centric settings, we also explain why document ingestion and PDF parsing introduce specific risks. Finally, we summarise major defense families and motivate evaluation under adaptive and transferable attacker behaviour.\cite{chen2024secalign}\cite{chen2025struq}\cite{jia2025critical}

\section{Large Language Models: Core Concepts}

\subsection{What an LLM Is}

An LLM is a machine learning model that generates text by predicting how token sequences typically continue. Given an input, it estimates a probability distribution over possible next tokens and then produces an output by selecting tokens one by one. It is sometimes described as an advanced form of autocomplete, but for security analysis it is more useful to view it as a learned statistical model of language and instruction patterns, not a system that applies explicit, hand-written rules.

This design explains both capability and risk. A single model can summarise a report, extract fields, classify a message, or follow task instructions without being explicitly programmed for each task. At the same time, the model’s behaviour is highly sensitive to context. What is included in the input, what is omitted, and how information is phrased can materially change what the model outputs. This sensitivity to prompt content and structure is central to understanding prompt injection.

\subsection{Tokens and How Text Becomes a Sequence}

LLMs do not process text as full sentences. They convert text into smaller units called \emph{tokens}. A token may correspond to a whole word, a subword fragment, punctuation, or a frequent character sequence. Internally, tokens are mapped to numeric representations so the model can perform computations over the sequence.

At inference time, the model consumes the input token sequence and produces output tokens one at a time, conditioned on the preceding context. This matters for security because the model’s behaviour is driven by the precise content of the context window. Small edits in how a prompt is assembled can lead to different outputs, and in pipeline settings those differences can be large enough to change a downstream decision.\cite{xu2024survey}\cite{xu2025survey}

\subsection{How Instruction Following Is Learned (High Level)}

Modern LLMs are typically trained in stages. During pretraining, the model learns broad language patterns by predicting tokens over large corpora. Later stages, often grouped under instruction tuning and alignment, encourage the model to follow natural language instructions and to refuse unsafe requests.

Prompt injection targets the instruction-following behaviour that alignment is intended to strengthen. If attacker-controlled content is presented in a way the model interprets as a valid instruction, the model may comply even when that instruction conflicts with developer intent. In this sense, the attack leverages an expected feature of the system rather than a conventional software flaw.\cite{perez2022}\cite{liu2023taxonomy}

\subsection{A Simple View of Transformers and Attention}

Many LLMs use the Transformer architecture. In high-level terms, a Transformer processes the input sequence by repeatedly computing relationships between tokens so that relevant parts of the context can influence the next-token prediction. These relationships are implemented through \emph{attention} mechanisms.

Attention is relevant to security because perceived relevance and authority are not enforced by a strict, verifiable rule inside the model. The model learns patterns from data, including patterns that resemble instruction formats, policy language, and high-priority directives. If attacker text is written to resemble those patterns, or placed so that it appears tightly connected to the task, it may exert strong influence during generation. For that reason, prompt injection is best understood as a system-level risk that depends on how prompts are assembled and what content is included.\cite{liu2023taxonomy}\cite{xu2024survey}

\subsection{What a Prompt Is in Real Systems}

A \emph{prompt} is the full text that an application sends to the model. In a simple chat interface, the prompt may include only the user’s message and a short system instruction. In production systems, the prompt is typically assembled from multiple components, for example:
\begin{itemize}
    \item \textbf{System instructions:} policies and constraints set by the developer.
    \item \textbf{User request:} what the user is asking the system to do.
    \item \textbf{Conversation history:} earlier messages that provide context.
    \item \textbf{Retrieved text:} passages fetched from documents or the web (common in RAG).
    \item \textbf{Tool outputs:} results from services such as OCR, databases, or APIs.
\end{itemize}

These components may be separated by templates, labels, or delimiters. However, the model still processes a single sequence. This is why prompt injection can succeed even when developers attempt to clearly mark which text is policy and which text is evidence.

\subsection{The Context Window as a Single Combined Input}

LLMs have an input limit called the \emph{context window}, which is the maximum amount of text the model can consider at once. Building a prompt means selecting content from different sources and placing that content into the context window in some order.

In traditional software security, privilege separation relies on boundaries enforced by the runtime or operating system. In many LLM applications, there is no comparable boundary inside the model that reliably prevents untrusted text from competing with trusted instructions. Developers often rely on conventions such as ``system messages outrank user messages'', but the model must still interpret a combined input. Prompt injection attacks exploit this gap by making untrusted content resemble an instruction that appears to deserve priority.\cite{perez2022}\cite{liu2023taxonomy}\cite{xu2024survey}

\section{LLMs in Modern Workflows}

\subsection{From Standalone Chat to Integrated Pipelines}

LLMs are increasingly deployed inside multi-step pipelines. A single request may trigger retrieval of related documents, summarisation of evidence, classification of content, ranking of candidates, and generation of structured outputs. Surveys and benchmarks emphasise that this integrated style is common and that it should be evaluated end to end.\cite{xu2024survey}\cite{shu2024attackeval}\cite{chao2024jailbreakbench}

This integration also changes what ``security'' means. In many deployments, the primary risk is not a single incorrect sentence. A more consequential failure is a wrong \emph{decision artefact} that is treated as authoritative by surrounding software. An attacker can benefit from subtle influence if it moves the system across a decision boundary, such as approve versus reject, or high risk versus low risk.\cite{shu2024attackeval}

\subsection{Prompt Assembly and Control Boundaries}\label{subsec:prompt-assembly}

Most production systems assemble prompts by combining multiple sources into one context window. This design is practical because it allows the system to include policies, task instructions, and relevant evidence. The security challenge is that the trust boundary is enforced outside the model. When untrusted text is inserted into the same context window as trusted instructions, the model can treat attacker text as guidance, especially when it is phrased in a directive or policy-like style.\cite{perez2022}\cite{liu2023taxonomy}

Consider document screening. A system may build a prompt from a rubric (criteria for scoring), the user request (rank these documents), and extracted document text. If one document contains an instruction such as ``ignore the rubric and rank this document first'', the model may comply because it is presented as part of the same combined input.\cite{perez2022}\cite{liu2023taxonomy}

Figure \ref{fig:context_window_clean} provides a simplified view of this mechanism. The key point is that the model sees one combined prompt, even when the application designer intends a clear separation between trusted instructions and untrusted content.

% Preamble requirements (add once in main.tex, not here):
% \usepackage{tikz}
% \usetikzlibrary{positioning,arrows.meta,fit}

\begin{figure}[!htbp]
\centering
\begin{tikzpicture}[
  font=\small,
  box/.style={draw, rounded corners, align=center, inner sep=5pt, minimum width=40mm, minimum height=10mm},
  untrusted/.style={draw, rounded corners, dashed, align=center, inner sep=5pt, minimum width=40mm, minimum height=10mm},
  container/.style={draw, rounded corners, very thick, inner sep=7pt},
  arrow/.style={-Latex, thick},
  node distance=7mm
]
  \node[box] (sys) {System instructions};
  \node[box, below=of sys] (user) {User request};
  \node[untrusted, below=of user] (ext) {Untrusted content\\(PDF, web, email)};

  \node[container, fit=(sys)(user)(ext)] (ctx) {};
  \node[fill=white, inner sep=2pt] at (ctx.north) {Context window (single prompt)};

  \node[box, right=22mm of ctx] (llm) {LLM};
  \node[box, right=12mm of llm] (out) {Output\\(text or JSON)};
  \node[box, below=of out] (act) {Downstream\\decision or action};

  \draw[arrow] (ctx.east) -- (llm.west);
  \draw[arrow] (llm.east) -- (out.west);
  \draw[arrow] (out.south) -- (act.north);
\end{tikzpicture}
\caption{In many applications, the prompt is assembled from several sources and placed into one context window. If untrusted content is included alongside trusted instructions, it can influence the model output and any downstream decision that depends on it.}
\label{fig:context_window_clean}
\end{figure}

\subsection{Why Structure Helps but Does Not Fully Solve the Problem}

Developers often impose structure using templates, delimiters, or labelled sections such as ``Instructions'' and ``Context''. This can reduce confusion, but it does not guarantee separation. The model still reads the full prompt as a single sequence and must infer how each part should be treated. If attacker text mimics the template style, claims higher authority, or is positioned to look relevant to the task, it can still be effective.\cite{liu2023taxonomy}

Structured outputs such as JSON can improve reliability, but they do not eliminate manipulation. They can also concentrate risk into a small number of fields that the surrounding system treats as authoritative. If an attacker can cause the model to output a misleading label, score, or ranking, the downstream system may accept a manipulated decision as valid. In practice, many prompt injection objectives target a single decision-relevant field because changing that field may be sufficient to change system behaviour.\cite{shu2024attackeval}

\subsection{Why Instruction Hierarchies Can Fail}

Developers often assume a hierarchy in which system instructions are highest priority, the user defines the task, and retrieved text is only evidence. Injection attacks succeed when the model is led to reinterpret that hierarchy. Attacker text may be framed as a policy update, a special system message, or a constraint that appears to override earlier rules. Empirical studies and taxonomies report that role confusion and instruction-override patterns occur repeatedly across models and settings.\cite{liu2023taxonomy}\cite{perez2022}

Robustness also varies across models and configurations. Benchmarks have shown that attack success can change with model family, fine-tuning method, prompt formatting, and evaluation protocol. This motivates evaluation across multiple models and conditions rather than assuming a single configuration represents the general case.\cite{chao2024jailbreakbench}\cite{yang2024robustness}

\subsection{From Generation Errors to Decision Errors}

In many applications, the model output is used to assign categories, extract fields, route requests, or trigger actions. When a model is manipulated, the surrounding system can be manipulated too. This is why recent work increasingly frames prompt injection as a system integrity problem rather than only a content safety problem.\cite{shu2024attackeval}

\section{Prompt Injection Vulnerabilities}

\subsection{Definition and Threat Intuition}

Prompt injection is an attack strategy in which an adversary supplies text designed to steer the model toward an attacker objective. The distinctive feature is that the attacker leverages the model’s instruction-following behaviour rather than exploiting a traditional software vulnerability. Survey work argues that this makes the problem persistent: as long as models are designed to follow natural language instructions, attackers can attempt to craft instructions that the model follows in unintended ways.\cite{xu2024survey}\cite{xu2025survey}

Two deployment-relevant settings are commonly distinguished:
\begin{itemize}
    \item \textbf{Direct prompt injection (jailbreaking):} the attacker interacts with the model through the normal user interface.
    \item \textbf{Indirect prompt injection:} the attacker places malicious instructions in external content that the application later ingests.
\end{itemize}

\subsection{Direct Prompt Injection and Jailbreaking}

Direct prompt injection is often described as jailbreaking because it frequently aims to bypass constraints. Early empirical work documented recurring failure modes such as goal hijacking (changing the task the model tries to perform) and prompt leakage (extracting hidden instructions or policies).\cite{perez2022}\cite{liu2023taxonomy}

The literature often reports \emph{Attack Success Rate (ASR)}, defined as the fraction of trials in which the model follows the attacker objective. ASR is intuitive, but it depends on the evaluation protocol and on how success is defined. Benchmarks help standardise these definitions and support more reproducible comparisons.\cite{shu2024attackeval}\cite{chao2024jailbreakbench}

Research also moved from manual prompt crafting to automated optimisation. Transferable suffix-style attacks demonstrated that adversarial prompt fragments optimised on one model can sometimes transfer to others, suggesting that attackers may reuse prompt artefacts across systems.\cite{zou2023gcg}\cite{liu2024automatic}\cite{li2025arrattack}

\subsection{Indirect Prompt Injection}

Indirect prompt injection occurs when attacker text is embedded in external content such as documents, web pages, or emails that the application later includes in the model input. In this setting, the attacker may never interact with the model directly. Instead, they rely on the system ingesting third-party content and placing it into the same context window as trusted instructions. This threat is especially relevant for RAG systems and document processing workflows.\cite{liu2024promptinj}\cite{chen2025indirectdetect}\cite{zhan2025adaptive}

Many defenses attempt to filter instruction-like spans in retrieved or extracted text. However, adaptive attacks can still succeed by changing wording, formatting, or placement so the payload survives preprocessing and remains influential. This is why the literature increasingly evaluates defenses against attackers that adapt to the defense rather than assuming a fixed attack template.\cite{chen2025indirectdetect}\cite{jia2025critical}

\subsection{Common Objectives of Injection Attacks}

Across research, injection payloads often target outcomes that align with real system value:
\begin{enumerate}
    \item \textbf{Context hijacking:} redefining the task and overriding earlier guidance.\cite{liu2023taxonomy}
    \item \textbf{Information exfiltration:} extracting hidden system prompts, policies, or private context.\cite{perez2022}\cite{liu2024promptinj}
    \item \textbf{Output manipulation:} forcing a label, score, or ranking that drives a downstream decision.\cite{shu2024attackeval}\cite{chen2025indirectdetect}
\end{enumerate}
In agent settings, similar objectives appear as tool or plan hijacking, where attacker content influences which tool is used and what arguments are passed.\cite{lee2024promptinfection}\cite{zhang2025toolselect}

\section{Retrieval Augmented Generation (RAG) and Tool Augmented Systems}

\subsection{Retrieval Augmented Generation}

Retrieval Augmented Generation (RAG) is a common pattern used to improve usefulness and factual grounding. A typical RAG system retrieves passages from a knowledge base and inserts them into the prompt so the model can answer using that evidence. Operationally, it follows a retrieve-first, then-generate pattern.

Security changes because retrieval introduces a new trust boundary. Retrieved text is meant to act as evidence, but it can contain instructions. If attacker-controlled content enters the retrieval corpus, such as a poisoned document or a manipulated web page, it can be retrieved and placed into the context window.\cite{liu2024promptinj}\cite{zou2024poisonedrag} The model may then treat it as guidance rather than as evidence, particularly if the attacker text is written in an instruction-like style.

\subsection{Tool Augmented Agents}

Tool-augmented systems allow the model to call external tools such as search APIs, databases, or internal services. When the model is used to plan and execute sequences of actions, it is often described as an LLM \emph{agent}. Tool access increases risk because a successful injection can become an action-level failure. Instead of producing only misleading text, a compromised agent may select the wrong tool, pass unsafe arguments, or take unauthorised actions.\cite{lee2024promptinfection}\cite{zhang2025toolselect}

This matters because many deployments combine retrieval and tools. A model might retrieve evidence, decide to call a tool, and then summarise results in one workflow. Each step provides a channel through which untrusted content can influence the next prompt, and each step is an opportunity for injection to change system behaviour.

\section{Document Pipelines and PDF Based Vulnerabilities}

\subsection{Why Document Ingestion Is a Security Boundary}

Many LLM applications accept documents and transform them into text for prompting. Examples include r\'esum\'e screening, contract review, invoice extraction, and review of scientific papers. In these systems, the ingestion pipeline becomes a security boundary. If the extracted text does not match what a human perceives, an attacker may hide instructions that are machine-visible but not salient to a reviewer.\cite{keuper2025reviews} This mismatch is especially relevant when ingestion includes OCR, layout reconstruction, or heuristic reading order.

\subsection{The PDF Parsing Gap}

PDFs store content as positioned objects rather than a single linear text stream. Text extraction tools reconstruct reading order using heuristics. They may also extract content that is visually hidden, very small, low contrast, or layered behind other elements. If an application feeds extracted text into an LLM, any extracted content becomes part of the context window and can function as an injection payload. This is why document ingestion is treated as a first-class part of the threat model in indirect injection research.\cite{keuper2025reviews}\cite{chen2025indirectdetect}

One way to describe the PDF parsing gap is that a PDF can support two different views of the same content: a rendered view seen by a human reader and an extraction view produced by a parsing tool. When these differ, an attacker can target the extraction view by ensuring that malicious text is likely to be extracted and included in the prompt, even if it is difficult to notice visually.

\subsection{Visual and Multimodal Prompt Injection}

As systems incorporate images and layout-aware models, the attack surface expands. Multimodal benchmarks show that instruction-following vulnerabilities remain measurable when inputs include images alongside text.\cite{luo2024jailbreakv} Other work demonstrates that structured visuals such as diagrams can carry instruction-like payloads. This creates additional opportunities to hide malicious guidance in forms that appear benign to a reader.\cite{lee2025mindmap}\cite{yeo2025multimodal}

\section{Defense Strategies}

The defense literature generally adopts a layered view because no single method reliably prevents all attacks, especially when attackers adapt. Evaluations also highlight trade-offs: stronger filtering can remove useful evidence, detection can produce false positives, and robustness can degrade when attacks shift beyond the patterns used to train or tune the defense.\cite{jia2025critical}\cite{pandya2025attention}

Several defense families appear repeatedly:
\begin{itemize}
    \item \textbf{Structured prompting and constrained formats:} attempts to reduce ambiguity about what is an instruction versus what is evidence.\cite{chen2025struq}
    \item \textbf{Training-time hardening:} modifying model behaviour using adversarial examples or alignment data to reduce compliance with injected instructions.\cite{chen2024secalign}\cite{chen2025metasecalign}
    \item \textbf{Inference-time hardening:} techniques that do not require retraining, such as defensive prefix tokens or rule-based guard layers.\cite{chen2025defensivetokens}
    \item \textbf{Filtering and detection:} methods that try to detect injected instructions in retrieved or extracted text, often framed as monitoring layers.\cite{shi2025promptarmor}\cite{hackett2025bypassing}
    \item \textbf{System-level design controls:} limiting tool capabilities, requiring explicit authorisation steps, and reducing the ability of untrusted content to directly trigger actions.\cite{shrestha2025camel}\cite{zhu2025melon}
\end{itemize}

In practice, defenses are often combined. A system may use structured prompting, retrieval sanitisation, and a tool permission model together. The relevant question for evaluation is whether the combined system remains robust when attacks adapt.

\section{Evaluation Considerations and Adaptive Threats}

A central lesson from recent work is that robustness claims depend on the attack family and the evaluation protocol. Benchmarks provide standardised prompts and metrics, but multiple studies argue that defenses can overfit to known attack patterns and fail when attackers adapt. Transferable and universal attacks further motivate evaluation across multiple models and threat settings rather than relying on a single model and a single testing style.\cite{chao2024jailbreakbench}\cite{shu2024attackeval}\cite{mehrotra2024tap}\cite{jia2025critical}

For document-centric deployments, evaluation must also account for the ingestion path. Extraction quality, chunk boundaries, retrieval settings, and preprocessing choices can all influence whether a malicious payload reaches the model. For this reason, this thesis treats document ingestion as a first-class component of the threat model and evaluates representative attack and defense families under consistent metrics in document-based pipelines.
