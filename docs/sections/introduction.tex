\chapter{Introduction}
\label{chap:intro}

Large Language Models (LLMs) have moved rapidly from experimental chat interfaces to practical components inside real software systems. Enterprises now use them to summarise documents, answer questions over internal knowledge bases, extract structured fields from forms, and support workflows that involve approvals, ranking, and routing. As a result, LLM outputs are increasingly treated as decision artefacts, such as a score, a label, a shortlist, or a recommended action, rather than as free-form text.

Embedding LLMs into such pipelines changes the security problem. In a deployed system, the model rarely receives only the user request. Instead, the application typically assembles a single model input by combining text from multiple sources, such as developer instructions, a user query, retrieved passages, and tool outputs. Some of these sources are trusted by design, while others are untrusted or only partially trusted, such as uploaded documents, web pages, emails, or OCR and extraction outputs. Because the model processes the entire prompt as one sequence, it must infer which text is authoritative instruction and which text is merely content. Prompt injection attacks exploit this ambiguity by placing attacker-controlled instructions into the same input, written in a way that resembles high-priority guidance, so that the model follows the attacker objective instead of the intended policy.

While much prior work discusses direct conversational jailbreaks, modern deployments increasingly rely on document ingestion, where the ingestion process itself becomes a security boundary. This thesis focuses on indirect prompt injection in document-centric workflows, where malicious instructions are embedded inside documents that the system later parses and forwards into the model prompt. In these settings, the attacker does not need interactive access to the model. They only need an opportunity to contribute content that the system will ingest, such as a PDF uploaded during a screening workflow or a document stored in a retrieval corpus.

PDF pipelines are especially relevant because PDF files do not store text as a simple linear stream. They store content as positioned objects, and extraction tools reconstruct reading order using heuristics. This creates a parsing gap in which the text extracted for machine processing can differ from what a human reviewer sees on the page. Low-visibility spans, unusual layout placements, or layered content may be captured by parsers and then included in prompts as if they were ordinary evidence. When this happens, hidden payloads can survive preprocessing and become indistinguishable from legitimate document content inside the model input. This makes document ingestion a first-class component of the threat model, rather than a minor implementation detail.

To study these risks under realistic conditions, this thesis introduces an empirical evaluation workflow, implemented as a research platform called InjectSuite. The system is designed to evaluate open-source LLMs under consistent metrics across both direct and indirect injection scenarios, with a specific emphasis on PDF document pipelines. InjectSuite supports controlled variation in factors that matter in practice, including document segmentation choices, payload placement and obfuscation variants, and prompt assembly structure. It also enables evaluation of layered defenses, such as structured prompting and input filtering, with the explicit goal of testing robustness under adaptive attackers rather than static templates. The overall objective is to make prompt-injection evaluation more reproducible and easier to extend, so that new models, defenses, and document types can be compared under a shared experimental harness.

The remainder of this thesis is organised as follows. Chapter~\ref{chap:background} introduces the foundations needed to understand the threat, including how LLMs process tokens, how prompts are assembled in real systems, and why document ingestion creates special risks. Chapter~\ref{chap:literature-review} surveys prior work on prompt injection and jailbreak attacks, indirect injection in retrieval and agent pipelines, and the main defense families used in practice. Chapter~\ref{chap:implementation} describes the architecture and implementation of InjectSuite, including attack generation, PDF segmentation, defense layers, and the evaluation pipeline. Chapter~\ref{chap:results} presents empirical findings across attacks, models, and defenses. Finally, Chapter~\ref{chap:conclusion} discusses implications for secure LLM deployments and directions for future research.
